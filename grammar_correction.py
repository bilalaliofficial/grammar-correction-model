# -*- coding: utf-8 -*-
"""Grammar Correction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w5uEXUlMVbOfGQbW_7nt81yK5w4HjZLO

# **Data Preprocessing**
"""

import pandas as pd

data = pd.read_excel('/content/Grammar Correction.xlsx')

data.head()

input_sentences = data['Ungrammatical Statement'].tolist()
output_sentences = data['Standard English'].tolist()

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenizing the sentences
input_tokens = [tokenizer.encode(sentence, add_special_tokens=True, max_length=128, padding='max_length', truncation=True) for sentence in input_sentences]
output_tokens = [tokenizer.encode(sentence, add_special_tokens=True, max_length=128, padding='max_length', truncation=True) for sentence in output_sentences]

from sklearn.model_selection import train_test_split

train_inputs, test_inputs, train_outputs, test_outputs = train_test_split(input_tokens, output_tokens, test_size=0.2, random_state=42)
train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(train_inputs, train_outputs, test_size=0.25, random_state=42)

"""# **Exploratory Data Analysis (EDA):**"""

import matplotlib.pyplot as plt

error_counts = data['Error Type'].value_counts()
plt.figure(figsize=(12, 6))
plt.bar(error_counts.index, error_counts.values)
plt.title('Distribution of Error Types')
plt.xlabel('Error Types')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

lengths = [len(sent.split()) for sent in data['Ungrammatical Statement']]
plt.hist(lengths, bins=20)
plt.title('Sentence Length Distribution')
plt.xlabel('Number of Words')
plt.ylabel('Count')
plt.show()

"""# **Baseline Machine Learning Model Bert:**"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(ngram_range=(1, 2))
X = vectorizer.fit_transform(data['Ungrammatical Statement'])

!pip install -q datasets==2.5.2

from transformers import EncoderDecoderModel, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import Dataset

train_dataset = Dataset.from_dict({'input_ids': train_inputs, 'labels': train_outputs})
val_dataset = Dataset.from_dict({'input_ids': val_inputs, 'labels': val_outputs})

model = EncoderDecoderModel.from_encoder_decoder_pretrained("bert-base-uncased", "bert-base-uncased")

model.config.decoder_start_token_id = tokenizer.cls_token_id
model.config.pad_token_id = tokenizer.pad_token_id

training_args = Seq2SeqTrainingArguments(
    output_dir='./models/',
    evaluation_strategy='epoch',
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=3,
    predict_with_generate=True
)
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()

from nltk.translate.bleu_score import sentence_bleu
from datasets import Dataset
import torch
from transformers import BertTokenizer, EncoderDecoderModel, pipeline
import numpy as np

test_dataset = Dataset.from_dict({'input_ids': test_inputs, 'labels': test_outputs})

model.eval()

generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0, decoder_start_token_id=tokenizer.cls_token_id)

test_results = test_dataset.map(lambda x: {"predicted": generator(tokenizer.decode(x['input_ids'], skip_special_tokens=True), max_length=128)[0]['generated_text']})

references = [tokenizer.decode(g, skip_special_tokens=True) for g in test_dataset['labels']]
hypotheses = [pred['predicted'] for pred in test_results]

bleu_scores = [sentence_bleu([ref.split()], hyp.split()) for ref, hyp in zip(references, hypotheses)]
average_bleu = np.mean(bleu_scores)
print(f"Average BLEU Score: {average_bleu}")

from sklearn.metrics import accuracy_score, f1_score

# Convert predictions to binary labels for the purpose of F1 and accuracy calculation
binary_references = [1 if ref == hyp else 0 for ref, hyp in zip(references, hypotheses)]
binary_hypotheses = [1 if len(hyp.split()) > 0 else 0 for hyp in hypotheses]  # Assume prediction of any text is 'attempt of correction'

# Calculate accuracy and F1 score for binary labels
accuracy = accuracy_score(binary_references, binary_hypotheses)
f1 = f1_score(binary_references, binary_hypotheses)

print(f"Accuracy: {accuracy}")
print(f"F1 Score: {f1}")

"""# **Generative AI Model Fine tuning**"""

from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=128)

data['text'] = data['Ungrammatical Statement'] + " <correct> " + data['Standard English']
dataset = Dataset.from_pandas(data)
dataset = dataset.map(tokenize_function, batched=True)

train_dataset, test_dataset = dataset.train_test_split(test_size=0.1).values()

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

model = GPT2LMHeadModel.from_pretrained('gpt2')

training_args = TrainingArguments(
    output_dir="./models/",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=16,
    weight_decay=0.01,
    save_total_limit=2,
    num_train_epochs=3,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

trainer.train()

from transformers import pipeline

generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)

text = "She no go to the park every day."

print("Beam Search:")
print(generator(text, max_length=50, num_beams=5, early_stopping=True)[0]['generated_text'])

print("\nSampling:")
print(generator(text, max_length=50, do_sample=True, top_k=50)[0]['generated_text'])

print("\nTemperature Adjusted Sampling:")
print(generator(text, max_length=50, do_sample=True, temperature=0.7)[0]['generated_text'])

print(test_dataset[0])

from nltk.translate.bleu_score import sentence_bleu

output_texts = [generator(tokenizer.decode(inp), max_length=160)[0]['generated_text'] for inp in test_dataset['input_ids']]
references = [tokenizer.decode(out, skip_special_tokens=True) for out in test_dataset['labels']]

bleu_scores = [sentence_bleu([ref.split()], out.split()) for ref, out in zip(references, output_texts)]
average_bleu = sum(bleu_scores) / len(bleu_scores)

print("Average BLEU Score:", average_bleu)