# -*- coding: utf-8 -*-
"""GenerativeAI_GrammerCorrection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qw9Uh_5kMqMoLNWWRUQx_uyOUcWLi2Sv

# **Data Preprocessing**
"""

import pandas as pd

data = pd.read_excel("Grammar Correction.xlsx")

data.head()

data.isnull().sum()

inputs = data['Ungrammatical Statement']
outputs = data['Standard English']

for i in range(5):
    print(f"Input: {inputs[i]} -> Output: {outputs[i]}")

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenized_inputs = [tokenizer.tokenize(sent) for sent in inputs]
tokenized_outputs = [tokenizer.tokenize(sent) for sent in outputs]

print(tokenized_inputs[:5])
print(tokenized_outputs[:5])

"""# **Exploratory Data Analysis (EDA):**"""

import matplotlib.pyplot as plt

error_counts = data['Error Type'].value_counts()

# Plot the distribution of error types
plt.figure(figsize=(12, 8))
error_counts.plot(kind='bar')
plt.title('Distribution of Grammatical Error Types')
plt.xlabel('Error Type')
plt.ylabel('Frequency')
plt.xticks(rotation=45, ha='right')
plt.show()

input_lengths = data['Ungrammatical Statement'].apply(lambda x: len(x.split()))
output_lengths = data['Standard English'].apply(lambda x: len(x.split()))

# Plot the distribution of sentence lengths
plt.figure(figsize=(12, 6))
plt.hist(input_lengths, bins=30, alpha=0.7, label='Ungrammatical Statements')
plt.hist(output_lengths, bins=30, alpha=0.7, label='Corrected Statements')
plt.title('Distribution of Sentence Lengths')
plt.xlabel('Sentence Length (number of words)')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""# **Baseline Machine Learning Model (LSTM):**"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer()
tokenizer.fit_on_texts(list(data['Ungrammatical Statement']) + list(data['Standard English']))

input_sequences = tokenizer.texts_to_sequences(data['Ungrammatical Statement'])
output_sequences = tokenizer.texts_to_sequences(data['Standard English'])

input_sequences = pad_sequences(input_sequences, padding='post')
output_sequences = pad_sequences(output_sequences, padding='post')

input_train, input_val, output_train, output_val = train_test_split(input_sequences, output_sequences, test_size=0.2, random_state=42)

decoder_input_data = pad_sequences([seq[:-1] for seq in output_sequences], maxlen=output_sequences.shape[1], padding='post')
decoder_output_data = pad_sequences([seq[1:] for seq in output_sequences], maxlen=output_sequences.shape[1], padding='post')

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.optimizers import Adam

vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
lstm_units = 256

# Define the model architecture
input_layer = Input(shape=(None,))
embedding_layer = Embedding(vocab_size, embedding_dim)(input_layer)

# Encoder
encoder_lstm = LSTM(lstm_units, return_state=True)
encoder_outputs, state_h, state_c = encoder_lstm(embedding_layer)
encoder_states = [state_h, state_c]

# Decoder
decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True)
decoder_inputs = Input(shape=(None,))
decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)
decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)
decoder_dense = Dense(vocab_size, activation='softmax')
output_layer = decoder_dense(decoder_outputs)

model = Model([input_layer, decoder_inputs], output_layer)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
model.summary()

import numpy as np
from sklearn.model_selection import train_test_split

decoder_input_data_train, decoder_input_data_val = train_test_split(decoder_input_data, test_size=0.3, random_state=42)
decoder_output_data_train, decoder_output_data_val = train_test_split(decoder_output_data, test_size=0.5, random_state=42)

# Training the model
model.fit([input_train, decoder_input_data_train], np.expand_dims(decoder_output_data_train, -1),
          validation_data=([input_val, decoder_input_data_val], np.expand_dims(decoder_output_data_val, -1)),
          epochs=20, batch_size=64)

scores = model.evaluate([input_val, decoder_input_data_val], np.expand_dims(decoder_output_data_val, -1))
print(f"Accuracy: {scores[1]*100}%")

"""# **Generative AI Enhancement:**"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

def build_text_file(data, filename):
    with open(filename, 'w') as f:
        for input, output in zip(data['Ungrammatical Statement'], data['Standard English']):
            f.write(input + " [SEP] " + output + " [EOS]\n")

build_text_file(data, 'train_dataset.txt')

train_dataset = TextDataset(
    tokenizer=tokenizer,
    file_path='train_dataset.txt',
    block_size=128
)

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

training_args = TrainingArguments(
    output_dir='./gpt2-finetuned-grammar',
    overwrite_output_dir=True,
    num_train_epochs=10,
    per_device_train_batch_size=4,
    save_steps=10_000,
    save_total_limit=2,
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
)

trainer.train()

model.save_pretrained('./gpt2-finetuned-grammar')
tokenizer.save_pretrained('./gpt2-finetuned-grammar')

def extract_correction(output):
    if '[SEP]' in output:
        return output.split('[SEP]')[1].split('[EOS]')[0].strip()
    return output

for text in ["She no went to the market.", "The cake needs to be baked in 350Â°F for 30 minutes."]:
    correction = generator(text, max_length=60, num_return_sequences=1, num_beams=5, truncation=True)
    corrected_text = extract_correction(correction[0]['generated_text'])
    print(f"Original: {text}\nCorrected: {corrected_text}\n")

from nltk.translate.bleu_score import sentence_bleu

def calculate_bleu(reference, hypothesis):
    reference = [ref.split() for ref in reference]
    hypothesis = hypothesis.split()
    return sentence_bleu(reference, hypothesis)

sample_text = "She no went to the market."
reference_text = ["She did not go to the market."]
generated_text = generator(sample_text, max_length=60, num_return_sequences=1, num_beams=5)[0]['generated_text']
bleu_score = calculate_bleu(reference_text, generated_text)
print(f"BLEU score: {bleu_score}")

